{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('haarcascade_eye.xml')\n",
    "\n",
    "img = cv2.imread('170904120420_1_900x600.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow('image',img)\n",
    "faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "for (x,y,w,h) in faces:\n",
    "    img = cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    roi_gray = gray[y:y+h, x:x+w]\n",
    "    roi_color = img[y:y+h, x:x+w]\n",
    "    eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "    for (ex,ey,ew,eh) in eyes:\n",
    "        cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
    "\n",
    "cv2.imshow('img',img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "img = cv2.imread('170904120420_1_900x600.jpg') # load a dummy image\n",
    "cv2.imshow('img',img)\n",
    "k = cv2.waitKey(1000)\n",
    "if k==97:\n",
    "    print(\"a pressed\")\n",
    "else:\n",
    "    print(k)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret,img=cap.read()\n",
    "    cv2.imshow('Video',img)\n",
    "    if cv2.waitKey(10000): \n",
    "        cv2.destroyAllWindows()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img = cv2.imread('170904120420_1_900x600.jpg',0)\n",
    "plt.imshow(img, cmap = 'gray', interpolation = 'bicubic')\n",
    "plt.xticks([]), plt.yticks([])  # to hide tick values on X and Y axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('C:/Users/HP/Downloads/opencv/sources/data/haarcascades/haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('C:/Users/HP/Downloads/opencv/sources/data/haarcascades/haarcascade_eye.xml')\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        # Our operations on the frame come here\n",
    "        frame = cv2.flip(frame,1)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "        # Display the resulting frame\n",
    "        for (x,y,w,h) in faces:\n",
    "            cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "            roi_gray = gray[y:y+h, x:x+w]\n",
    "            roi_color = frame[y:y+h, x:x+w]\n",
    "            eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "            for (ex,ey,ew,eh) in eyes:\n",
    "                cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
    "        cv2.imshow('frame',frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the video output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('output.avi',fourcc, 20.0, (640,480))\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if ret==True:\n",
    "        frame = cv2.flip(frame,1)\n",
    "\n",
    "        # write the flipped frame\n",
    "        out.write(frame)\n",
    "\n",
    "        cv2.imshow('frame',frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Release everything if job is finished\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting full body in video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "# face_cascade = cv2.CascadeClassifier('C:/Users/HP/Downloads/opencv/sources/data/haarcascades/haarcascade_frontalface_default.xml')\n",
    "# eye_cascade = cv2.CascadeClassifier('C:/Users/HP/Downloads/opencv/sources/data/haarcascades/haarcascade_eye.xml')\n",
    "body_cascade = cv2.CascadeClassifier('C:/Users/HP/Downloads/opencv/sources/data/haarcascades/haarcascade_fullbody.xml')\n",
    "cap = cv2.VideoCapture('C:/Users/HP/Downloads/opencv/sources/samples/data/vtest.avi')\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        # Our operations on the frame come here\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = body_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "        # Display the resulting frame\n",
    "        for (x,y,w,h) in faces:\n",
    "            cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "            \n",
    "        cv2.imshow('frame',frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Captures the frame on double click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def capture_image(event,x,y,flags,param):\n",
    "    if event == cv2.EVENT_LBUTTONDBLCLK:\n",
    "        cv2.imwrite('capture.png',frame)\n",
    "#         cv2.circle(img,(x,y),100,(255,0,0),-1)\n",
    "        \n",
    "face_cascade = cv2.CascadeClassifier('C:/Users/HP/Downloads/opencv/sources/data/haarcascades/haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('C:/Users/HP/Downloads/opencv/sources/data/haarcascades/haarcascade_eye.xml')\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "cv2.namedWindow('frame')\n",
    "cv2.setMouseCallback('frame',capture_image)\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        # Our operations on the frame come here\n",
    "        frame = cv2.flip(frame,1)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "        # Display the resulting frame\n",
    "        for (x,y,w,h) in faces:\n",
    "            cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "            roi_gray = gray[y:y+h, x:x+w]\n",
    "            roi_color = frame[y:y+h, x:x+w]\n",
    "            eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "            for (ex,ey,ew,eh) in eyes:\n",
    "                cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
    "        cv2.imshow('frame',frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Live face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "\n",
    "# This is a demo of running face recognition on live video from your webcam. It's a little more complicated than the\n",
    "# other example, but it includes some basic performance tweaks to make things run a lot faster:\n",
    "#   1. Process each video frame at 1/4 resolution (though still display it at full resolution)\n",
    "#   2. Only detect faces in every other frame of video.\n",
    "\n",
    "# PLEASE NOTE: This example requires OpenCV (the `cv2` library) to be installed only to read from your webcam.\n",
    "# OpenCV is *not* required to use the face_recognition library. It's only required if you want to run this\n",
    "# specific demo. If you have trouble installing it, try any of the other demos that don't require it instead.\n",
    "\n",
    "# Get a reference to webcam #0 (the default one)\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Load a sample picture and learn how to recognize it.\n",
    "obama_image = face_recognition.load_image_file(\"obama.jpg\")\n",
    "obama_face_encoding = face_recognition.face_encodings(obama_image)[0]\n",
    "\n",
    "# Load a second sample picture and learn how to recognize it.\n",
    "biden_image = face_recognition.load_image_file(\"mehul.jpg\")\n",
    "biden_face_encoding = face_recognition.face_encodings(biden_image)[0]\n",
    "\n",
    "# Create arrays of known face encodings and their names\n",
    "known_face_encodings = [\n",
    "    obama_face_encoding,\n",
    "    biden_face_encoding\n",
    "]\n",
    "known_face_names = [\n",
    "    \"Barack Obama\",\n",
    "    \"Mehul Gupta\"\n",
    "]\n",
    "\n",
    "# Initialize some variables\n",
    "face_locations = []\n",
    "face_encodings = []\n",
    "face_names = []\n",
    "process_this_frame = True\n",
    "\n",
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    # Resize frame of video to 1/4 size for faster face recognition processing\n",
    "    small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "\n",
    "    # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
    "    rgb_small_frame = small_frame[:, :, ::-1]\n",
    "\n",
    "    # Only process every other frame of video to save time\n",
    "    if process_this_frame:\n",
    "        # Find all the faces and face encodings in the current frame of video\n",
    "        face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "        face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "\n",
    "        face_names = []\n",
    "        for face_encoding in face_encodings:\n",
    "            # See if the face is a match for the known face(s)\n",
    "            matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "            name = \"Unknown\"\n",
    "\n",
    "            # If a match was found in known_face_encodings, just use the first one.\n",
    "            if True in matches:\n",
    "                first_match_index = matches.index(True)\n",
    "                name = known_face_names[first_match_index]\n",
    "\n",
    "            face_names.append(name)\n",
    "\n",
    "    process_this_frame = not process_this_frame\n",
    "\n",
    "\n",
    "    # Display the results\n",
    "    for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "        # Scale back up face locations since the frame we detected in was scaled to 1/4 size\n",
    "        top *= 4\n",
    "        right *= 4\n",
    "        bottom *= 4\n",
    "        left *= 4\n",
    "\n",
    "        # Draw a box around the face\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "\n",
    "        # Draw a label with a name below the face\n",
    "        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "        font = cv2.FONT_HERSHEY_DUPLEX\n",
    "        cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "\n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Hit 'q' on the keyboard to quit!\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release handle to the webcam\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "im = Image.new(\"RGB\", (2400, 1700), \"white\")\n",
    "\n",
    "im2 = Image.open(\"sachinn.jpg\")\n",
    "im.paste(im2, (500,500))\n",
    "im.save(\"test.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image \n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('C:/Users/HP/Downloads/opencv/sources/data/haarcascades/haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('C:/Users/HP/Downloads/opencv/sources/data/haarcascades/haarcascade_eye.xml')\n",
    "\n",
    "# im2=Image.open(\"download.png\")\n",
    "im2=cv2.imread(\"download.png\")\n",
    "im2=cv2.resize(im2, (0,0), fx=0.2, fy=0.2) \n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "def overlay_image_alpha(img, img_overlay, pos, alpha_mask):\n",
    "    x, y = pos\n",
    "    # Image ranges\n",
    "    y1, y2 = max(0, y), min(img.shape[0], y + img_overlay.shape[0])\n",
    "    x1, x2 = max(0, x), min(img.shape[1], x + img_overlay.shape[1])\n",
    "\n",
    "    # Overlay ranges\n",
    "    y1o, y2o = max(0, -y), min(img_overlay.shape[0], img.shape[0] - y)\n",
    "    x1o, x2o = max(0, -x), min(img_overlay.shape[1], img.shape[1] - x)\n",
    "\n",
    "    # Exit if nothing to do\n",
    "    if y1 >= y2 or x1 >= x2 or y1o >= y2o or x1o >= x2o:\n",
    "        return\n",
    "\n",
    "    channels = img.shape[2]\n",
    "\n",
    "    alpha = alpha_mask[y1o:y2o, x1o:x2o]\n",
    "    alpha_inv = 1.0 - alpha\n",
    "\n",
    "    for c in range(channels):\n",
    "        img[y1:y2, x1:x2, c] = (alpha_inv * img_overlay[y1o:y2o, x1o:x2o, c] +\n",
    "                                alpha * img[y1:y2, x1:x2, c])\n",
    "        \n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        # Our operations on the frame come here\n",
    "        frame = cv2.flip(frame,1)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "#         pil_im = Image.fromarray(frame)\n",
    "        overlay_image_alpha(frame, im2[:, :, 0:3],(10,10),im2[:,:,2] / 255.0)\n",
    "        cv2.imshow('frame',frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# im1=cv2.imread(\"Snapshot_20180620_1.JPG\")\n",
    "# overlay_image_alpha(im1, im2[:, :, 0:3],(10,10),im2[:,:,2] / 255.0)\n",
    "# cv2.imshow('frame',im1)\n",
    "# cv2.waitKey(0)\n",
    "# When everything done, release the capture\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_im = Image.fromarray(frame)\n",
    "pil_im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.shape\n",
    "im2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, cv2\n",
    "\n",
    "img1 = cv2.imread()\n",
    "img2 = cv2.imread()\n",
    "h1, w1 = img1.shape[:2]\n",
    "h2, w2 = img2.shape[:2]\n",
    "vis = np.zeros((max(h1, h2), w1+w2), np.uint8)\n",
    "vis[:h1, :w1] = img1\n",
    "vis[:h2, w1:w1+w2] = img2\n",
    "vis = cv2.cvtColor(vis, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "cv2.imshow(\"test\", vis)\n",
    "cv2.waitKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding specs to video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils import face_utils\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import dlib\n",
    "import cv2\n",
    "import math\n",
    "from PIL import Image\n",
    "\n",
    "# face_cascade = cv2.CascadeClassifier(\n",
    "#     'C:/Users/HP/Downloads/opencv/sources/data/haarcascades/haarcascade_frontalface_default.xml')\n",
    "# eye_cascade = cv2.CascadeClassifier('C:/Users/HP/Downloads/opencv/sources/data/haarcascades/haarcascade_eye.xml')\n",
    "\n",
    "# im2=Image.open(\"download.png\")\n",
    "# im2 = cv2.imread(\"facial-landmarks/download.png\")\n",
    "# im2 = cv2.resize(im2, (0, 0), fx=0.2, fy=0.2)\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "# fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "# out = cv2.VideoWriter('facial-landmarks/output.avi',fourcc, 5.0, (640,480))\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "def overlay_image_alpha(img, img_overlay, pos, alpha_mask):\n",
    "    x, y = pos\n",
    "    # Image ranges\n",
    "    y1, y2 = max(0, y), min(img.shape[0], y + img_overlay.shape[0])\n",
    "    x1, x2 = max(0, x), min(img.shape[1], x + img_overlay.shape[1])\n",
    "\n",
    "    # Overlay ranges\n",
    "    y1o, y2o = max(0, -y), min(img_overlay.shape[0], img.shape[0] - y)\n",
    "    x1o, x2o = max(0, -x), min(img_overlay.shape[1], img.shape[1] - x)\n",
    "\n",
    "    # Exit if nothing to do\n",
    "    if y1 >= y2 or x1 >= x2 or y1o >= y2o or x1o >= x2o:\n",
    "        return\n",
    "\n",
    "    channels = img.shape[2]\n",
    "\n",
    "    alpha = alpha_mask[y1o:y2o, x1o:x2o]\n",
    "    alpha_inv = 1.0 - alpha\n",
    "\n",
    "    for c in range(channels):\n",
    "        img[y1:y2, x1:x2, c] = (alpha_inv * img_overlay[y1o:y2o, x1o:x2o, c] +\n",
    "                                alpha * img[y1:y2, x1:x2, c])\n",
    "\n",
    "\n",
    "# initialize dlib's face detector (HOG-based) and then create\n",
    "# the facial landmark predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"facial-landmarks/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# load the input image, resize it, and convert it to grayscale\n",
    "# image = cv2.imread(args[\"image\"])\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, image = cap.read()\n",
    "    if ret:\n",
    "        # Our operations on the frame come here\n",
    "        image = cv2.flip(image, 1)\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        # faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "        # detect faces in the grayscale image\n",
    "        rects = detector(gray, 1)\n",
    "\n",
    "        # loop over the face detections\n",
    "        for (i, rect) in enumerate(rects):\n",
    "            # determine the facial landmarks for the face region, then\n",
    "            # convert the facial landmark (x, y)-coordinates to a NumPy\n",
    "            # array\n",
    "            shape = predictor(gray, rect)\n",
    "            shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "            # convert dlib's rectangle to a OpenCV-style bounding box\n",
    "            # [i.e., (x, y, w, h)], then draw the face bounding box\n",
    "            (x, y, w, h) = face_utils.rect_to_bb(rect)\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "            # show the face number\n",
    "            cv2.putText(image, \"Face #{}\".format(i + 1), (x - 10, y - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "            x1 = shape[17][0]\n",
    "            x2 = shape[26][0]\n",
    "            y1 = shape[19][1]\n",
    "            y2 = shape[28][1]\n",
    "\n",
    "            pil_im = Image.open('download.png')\n",
    "\n",
    "            a_x1 = shape[42][0]\n",
    "            a_y1 = shape[42][1]\n",
    "            a_x2 = shape[45][0]\n",
    "            a_y2 = shape[45][1]\n",
    "\n",
    "            angle = math.degrees(math.atan((a_y2 - a_y1) / (a_x2 - a_x1)))\n",
    "            pil_im.rotate(-angle, expand=True).resize((abs(x2 - x1 + 20), abs(y2 - y1))).save('down.png')\n",
    "            im2 = cv2.imread(\"down.png\")\n",
    "            # im2 = cv2.resize(im2, (abs(x2 - x1), abs(y2 - y1)))\n",
    "            # shift = (x2 - x1) * math.sin(math.radians(angle)) / 2\n",
    "            # y_shift = (y2 - y1) * math.sin(math.radians(angle)) / 2\n",
    "            # print(x1,shift,y1)\n",
    "            overlay_image_alpha(image, im2, (shape[17][0] - 10, shape[17][1]), im2[:, :, 2] / 255.0)\n",
    "\n",
    "#         out.write(image)\n",
    "        cv2.imshow('frame', image)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# image = imutils.resize(image, width=500)\n",
    "# gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# loop over the (x, y)-coordinates for the facial landmarks\n",
    "# and draw them on the image\n",
    "# for index,(x, y) in enumerate(shape):\n",
    "#       cv2.putText(image, \"{}\".format(index+1), (x, y),cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0,0,0), 1)\n",
    "#       cv2.circle(image, (x, y), 1, (0, 0, 255), -1)\n",
    "\n",
    "\n",
    "# show the output image with the face detections + facial landmarks\n",
    "# cv2.imshow(\"Output\", image)\n",
    "# cv2.waitKey(0)\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "# out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
